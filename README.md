ðŸ§‘â€âš–ï¸ **Indian Legal Query Classifier using Fine-Tuned BERT**
This project demonstrates how to fine-tune a pre-trained BERT model to classify Indian legal queries using the Indian Law dataset and deploy it using Streamlit for real-time predictions and visualization.

ðŸ“Œ **Project Features**
-> Fine-tunes BERT on the Indian Law Q&A dataset
-> Classifies legal questions into appropriate response categories
-> Visualizes model attention with heatmaps
-> Interactive UI built with Streamlit

ðŸ“‚ **Folder Structure**
legal-query-classifier/
â”œâ”€â”€ app.py                   # Streamlit frontend
â”œâ”€â”€ requirements.txt         # Required Python packages
â”œâ”€â”€ indian_law_model/        # Saved model & tokenizer
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”œâ”€â”€ tokenizer_config.json
â”‚   â”œâ”€â”€ vocab.txt
â”‚   â””â”€â”€ label_classes.npy

âœ…**Step-by-Step Setup**
ðŸš€ Part 1: Train the Model in Google Colab
-> Open this notebook(ModelTraining.ipynb) in Google Colab and run the training code.
-> Save the model artifacts:
    #Save model and tokenizer
    model.save_pretrained("indian_law_model")
    tokenizer.save_pretrained("indian_law_model")
    
    #Save label classes for decoding predictions
    import numpy as np
    np.save("indian_law_model/label_classes.npy", le.classes_)

-> Download the model folder:
    from google.colab import files
    import shutil
    shutil.make_archive("indian_law_model", 'zip', "indian_law_model")
    files.download("indian_law_model.zip")

ðŸ’» Part 2: Set Up Streamlit App Locally
-> Extract the indian_law_model.zip into your project folder.
-> Create a new Python file named app.py and paste the code.
-> Create a requirements.txt file:
    streamlit
    torch
    transformers
    scikit-learn
    seaborn
    matplotlib
    numpy
-> Install dependencies:
    pip install -r requirements.txt
-> Run the app:
    streamlit run app.py
